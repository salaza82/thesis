%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{sec:icDM_intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Neutrinos are another astrophyical messenger than can travel long distances without interaction.
Uniquely, they interact less readily than photons especially above PeV energies.
Neutrinos thereofre provide another window through which we can perform dark matter searches.
Neutrinos come in three flabors and so this triples the multiplicity of the particles we are searching for.

Icecube has not done a DM annihilation analysis towards dwarf galaxies for a while. \todo{cite 2013 paper}.
This is in spite of the potentially crucial sensitivity afforded from neutrino spectral lines \todo{cite dan hooper and neutrino lines.}
A lot has changed in IC3 since that last analysis (we have more strings, we have much more sophisticated analysis methods, and the theory modeling has made significant leaps.)
Therefore it is time to finally do a DM search toward dSphs.
The hope is that by laying down the important statistical foundation as well, that this work can be meshed with gamma-ray data.
IceCube is sensitive to annihilating DM to the DM ranges above 1 TeV and can produce competitive results relative to gamma ray observatories in spectral models that produce sharp neutrino features.
The goal of this analysis is to perform a DM annihilation search using the new datasets NST.
The search will only be towards dwarf spheroidal galaxies (dSph).
These sources are known for their low backgrounds and high DM contents.
Since the dataset is sensitive to the north and south, as many dSph as possible will be included.
Additionally, with annihilation, these sources can be treated as point sources with little loss to sensitivity or model dependence on how the DM is distributed.
DM masses from 500 GeV to 100 PeV are considered for this analysis.
All standard model annihilation channels available from the HDMSpectra are studied in this analysis.

Additional work is done to extract the Likelihood profiles for each DM, source hypothesis so that these data can be combined with gamma-ray observatories.
This work is considered a separate project as the statistical treatment is unique from many IceCube analyses.
The wiki for [ the combined analysis] \todo{instead point to chapter}
This chapter presents the analysis work for IC3 for DM searches toward dSphs.
This section describes the various steps and features of the analysis.
It is structure first introduces the data and how it is treated, then systematic studies of the dwarves individually. Finally, the stacked analysis and results are presented.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset and Background}\label{sec:icDM_databgd}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This section enumerates the data and background methods used for IceCube's study of dSphs.
\Cref{sec:icDM_data} and \Cref{sec:icDM_tools} are most useful for fellow IceCube collaborators looking to replicate this analysis.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Itemized IceCube files}\label{sec:icDM_data}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
    \item Software Environment: \texttt{CVMFS Py3-v4.1.1}
    \item Data Sample: Northern Tracks \texttt{NY86v5p1}
    \item Analysis Software: cksy (\href{https://github.com/icecube/csky/tree/nu\_dark\_matter}{nu\_dark\_matter})
    \item Analysis wiki: \url{https://wiki.icecube.wisc.edu/index.php/Dark\_Matter\_Annihilation\_Search\_towards\_dwarf\_spheroidals\_with\_NST\_and\_DNN\_Cascades}
    \item \href{https://github.com/salaza82/IceCube_dark_matter_dsph}{Project repository}
\end{itemize}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Software Tools and Development}\label{sec:icDM_tools}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This analysis was performed inside IceCube's CVMFS (3.4.1.1) software environment using csky for likelihood calculations.
Csky did not come with dark matter spectral models nor could accomodate custom flux models.
We developed these capacities for single source and stacked source studies for this analysis.
The analysis code is held in a separate repository from csky.
The \texttt{nu\_dark\_matter} \href{https://github.com/icecube/csky/tree/nu\_dark\_matter}{branch of csky} manages the input of custom dark matter spectra and accompanied DM astrophysical source then calculates likelihoods with a selected data sample.
The \href{https://github.com/salaza82/IceCube_dark_matter_dsph}{IceCube Dark Matter dSph repository} manages the generation of spectral models for neutrinos, physics parameter exctraction from $n_{\mathrm{sig}}$, \J-factor per source inputs, and bookkeeping for the large parameter space.
The project repository required a secondary software environment for neutrino oscillations.
How to launch and run those calculations are documented in the project repository and the Docker image is additionally saved in \cref{sec:apdx_nu_spec}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Data Set and Background Description} \label{sec:icDM_data_bkgd}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

For this analysis, I use the Northern Sky Tracks (NST) Sample (Version V005-P01).
The sample contains up-going track-like events, usually from $\nu_\mu$ and $\nu_\tau$ and has a superior angular resolution compared to the cascade dataset.
This sample covers 10.4 years of data (IC86\_2011-2021).
The accepted neutrino energy range used for the analysis is unique from most other IceCube searches because DM spectra are very hard.
The sampled energy range is $1 < \log(E_\nu /\textrm{GeV}) < 9.51$ with step size 0.125.

The strength of a dwarf analysis is that there is no additional background consideration beyond nominal, baseline background estimations.
For NST, the nominal contribution comes from atmospheric neutrinos and isotropic astrophysical neutrinos.
We estimate the background by scrambling NST data along Right Ascension.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Analysis}\label{sec:icDM_analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The expected differential neutrino flux from DM-DM annihilation to standard model
particles, $d\Phi_{\nu}/dE_{\nu}$, over solid angle, $\Omega$~is described by the familiar equation.
\iddmannilation[\nu]

This is identical to past examples except that there are 3 neutrino flavors, so there are a corresponding 3 flux equations.
\cref{sec:gd_analysis} has a complete description of all the terms.
Additionaly, neutrinos oscillate between flavors which needs to be considered for the expected neutrino flux at Earth.
\Cref{sec:icDM_particlephysics} presents the particle physics model for DM annihilation.
\Cref{sec:icDM_spatialmodel} presents the spatial distributions built for each dSph.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{$\frac{dN_\nu}{dE_\nu}$ - Particle Physics Component}\label{sec:icDM_particlephysics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Neutrino spectra from heavy dark matter annihilation were generated using HDMSpectra \cite{HDMSpectra} and $\chi \textrm{aro}\nu$ \cite{Charon}.
HDMSpectra simulates the decay and annihilation of heavy dark matter, for different dark matter masses and SM primary annihilation channels.
The simulation includes electroweak radiative corrections and higher order loop corrections with quarks.
This publication also pushes the simulated DM mass to the Plank scale (1 EeV), however this study will not explore that high.

An important novel feature in the spectra is that neutrino line channels will be accompanied with a low energy tail.
Thus the earth will not fully attenuate a neutrino SM channel signal from high declination sources where the neutrino flux must first traverse through the Earth.
The SM annihilation channels that feature lines include all leptonic channels. ($\nu_{e,\mu,\tau}, e, \mu, \mathrm{and} \tau$)
We use \href{https://iopscience.iop.org/article/10.1088/1475-7516/2020/10/043}{ $\chi \mathrm{aro}\nu$} to propagate and oscillate the neutrinos from the source to Earth.
Because these sources are quite large in absolute terms, and also far (order 10 kpc or more), the resulting flavor spectra are the averages of the transition probabilities \cite{Charon}:
\nuOscMatrix

When calculating the expected contribution to $n_s$, only $ \nu_\mu, \nu_\tau $ are considered as NST's effective area to $ \nu_e $ is essentially 0.
With these consideration, the expected composite neutrino spectrum is a average of the two flavors: $ (\nu_\mu + \nu_\tau)/2 $.
The spectral tables are then converted to splines to condense information, enable random sampling of the spectra, and enable faster computation times.
The spectral splines are finally implemented as a DM class in csky.
Examples of the spectra before and after propagation are shown in \cref{fig:nu_osc_dm}.
\input{tables/nu_osc_dm.tex}

% ==== Treatment of line features ====
%
% All leptonic DM annihilation channels $ \chi\chi \rightarrow [\nu_{e, \mu, \tau}, e, \mu,\tau] $ develop a prominent and narrow spectral line feature.
% For all neutrino flavors, this line is visible and prominent in all mass models studied for this analysis.
% For charged leptons, the feature only really shows up at the larger DM mass models.
% This line feature is so narrow relative the sampled energy range that the MC rarely samples within the neutrino line.
% As a result, often the best fit to simulation of background will always floor to TS = 0 and the signal recovery tends to be conservatve.
%
% <center>
% [[Image:sig_recover_line.png | 350px]]
%
% Signal recovery for 100 TeV DM are Declination of 16.06. Although the uncertainties are small and tight, the reconstructed $ n_s $ are systematically underestimated.
% </center>
%
% To remedy this, a similar approach to the [https://wiki.icecube.wisc.edu/index.php/Search_for_Decaying_DM_in_Galaxy_Clusters_and_Galaxies decay analysis].
% 2 kernels were tested (Gaussian, uniform (flat)) to smooth out the line feature.
% The widths were tuned such that the signal recovery approached unity for DM mass 100 TeV to 1 PeV.
% Additionally, the tuning was performed only for a source at declination 16.06 (Segue 1).
% This is to avoid confusion loss in signal recovery from too narrow a line and from Earth's attenuation of high energy neutrinos.
% The convolution also needed to as close as possible preserve the integrated counts of neutrinos.
% The optimized kernel window for all lines is summarized as:
% * Guassian kernel w/$2 \sigma$ width = 3.5E-3$\cdot m_\chi$
% * Minimum energy included in convolution = MIN$[0.995 \cdot m_\chi, En(\nu_{line}) -4\sigma]$
% * Maximum energy included in convolution = MAX$[1.005 \cdot m_\chi, En(\nu_{line}) +4\sigma]$
% where $En(\nu_{line})$ is the neutrino energy where the neutrino line is at the maximum.
%
% These parameters broadly improved the signal recovery of the line spectra. An example is provided below. Signal recovery plots of the full analysis are provided much further down.
%
% <center>
% [[Image:improved_sig_recover_line.png | 350px]]
%
% Top left panel shows the two kernels overlayed the original spectrum from Charon. delta I is the difference in the integral of the peaks with respect to the original spectrum. The vertical red line indicated where the original neutrino line is maximized. Lower right shows the signal recovers of the DM model using the Gaussian kernel with parameters enumerated above.
%
% </center>
%
% ==== Spline Fitting ====
%
% In an effort to reduce computational work, memory burden, and align with point source methods used for NGC1068 and Seyfert analyses, spectral splines were created and adopted for estimating the neutrino flux for the different annihilation.
% Software was written to generate, handle, and calculate values on the splines.
% When using splines, one has to be careful of the goodness to fit.
% There are critical caveats when testing the goodness to fit to MC generated above for all channels.
% * The splines must be Log10(*) in Energy and dN/dE to acount for the exponential nature of the flux
% * The fidelity of the fit matters more at $ E_\nu \thickapprox m_\chi $ where the model uncertainties are minimal and physical considerations (like the cut-off) are most apparent.
% * The fidelity of the fit matters less at low $ E_\nu $ as the model uncertainties are large AND IceCube's sensitivity diminishes significantly below 500 GeV
% * Total integrated counts should be well preserved, however, the resolution of the MC is much higher than IceCube's energy resolution.
% :* Meaning over several steps in E, the integral is preserved
% :* the step size enters the cost function
% :* Oscillating residuals, so long as they are very small and well centered, are not penalized as this gets averaged out.
%
% The resulting cost function to evaluate the goodness of fit was used to account for the above considerations.
%
% <center>
% $
% e_i = x_i \cdot (\frac{dN_i}{dE_i} - 10^{\hat{e_i}})
% $
% </center>
%
% Where $ \hat{e_i} $ is the spline estimator's value for $x_i$. $ x_i = E_{\nu_i} / m_\chi $. $ \frac{dN_i}{dE_i} $ is the flux value from MC.
%
% <center>
% $
% \mathrm{err} = \sqrt{{\frac{1}{x_\mathrm{max}-x_\mathrm{min}}\int_{x_\mathrm{min}}^{x_\mathrm{max}} \mathbf{e}^2 dx}}
% $
% </center>
%
% I then take the RMS of the error distribution and the resulting value (err) is used to evaluate the fidelity of the spectral spline.
% Each SM channel had different tolerances for 'err'. Channels with very hard cut-offs had looser tolerance for err because a lot of error would be generated from the cut-off being estimated to occur slightly early or late.
% Soft channels don't have this issue and therefore the tolerance is very strict.
% The table blow summarizes the tolerances for the SM channels.
%
% {| class="wikitable"
% |-
% ! scope="col" style="width: 100px;" | $ \chi\chi \rightarrow $
% ! scope="col" style="width: 90px;" |  GOOD
% err < val
% ! scope="col" style="width: 100px;" | OK
% err : (val1, val2]
% ! scope="col" style="width: 90px;" | FAIL
% err > val
% ! Limits of err calc [X_min, X_max]
% |-
% | $ Z^0Z^0, W^+W^- $ || 1.0E-3 || 1.0E-3, 1.0E-2 || 1.0E-2 || MAX$[100 \mathrm{GeV}/m_\chi, 10^{-6}], 1.0 $
% |-
% | $ t\bar{t}, hh $ || 1.0E-5 || 1.0E-5, 1.0E-4 ||  1.0E-4 || MAX$[100 \mathrm{GeV}/m_\chi, 10^{-6}], 1.0 $
% |-
% | $ b\bar{b}, d\bar{d}, u\bar{u}$ || 9.0E-7 || 9.0E-7, 9.0E-6 || 9.0E-6 || MAX$[100 \mathrm{GeV}/m_\chi, 10^{-6}], 1.0 $
% |-
% | $ \nu\bar{\nu}_{e, \mu, \tau} $ || 1.0E-3 || 1.0E-3, 1.0E-2 || 1.0E-2 || MAX$[100 \mathrm{GeV}/m_\chi, 10^{-6}]$, MIN$[0.995, (En(\nu_{line}) -4\sigma)/M_{\chi}] $
% |-
% | | $ e\bar{e}, \mu\bar{\mu}, \tau\bar{\tau} $ || 1.0E-3 || 1.0E-3, 1.0E-2 || 1.0E-2 || MAX$[100 \mathrm{GeV}/m_\chi, 10^{-6}]$, MIN$[0.995, (En(\nu_{line}) -4\sigma)/M_{\chi}] $
% |+ Summary table of tolerances for splines used for spectral models.
% |}
%
% The errors are then plotted in two ways.
% First, FAIL and OK are directly plotted with e_i as a function of x, and the full spline and MC.
% Second, a summary plot of all the splines is plotted and colors coded.
%
% <center>
% [[Image:failed_spline.png | 700px]]
%
% Example spline that failed the fit. Failed splined are corrected on a case by case basis unless the SM channel has a systematic problem fitting the splines. In this case, I made a bookkeeping error and loaded the incorrected neutrino flavor
% </center>
%
% Below are the spline summaries and represent the current, up-to-date status of the splines.
% The goal broadly is to eliminate all red and inspect yellow. $ \nu_e $ is not considered in this analysis among the neutrino final states and so no work was done to converge the spline fits for this flavor.
%
% <center>
% [[Image:numu_spline_status.png | 700px]]
%
% Summary for $ \nu_{\mu} $ final state
%
% [[Image:nutau_spline_status.png | 700px]]
%
% Summary for $ \nu_{\tau} $ final state
% </center>
%
% A Final inspection of the splines by eye was done to verify that the spline fitting did not introduce spurious features into the distribution that would corrupt the LLH fitting.
%
% ==== Final Spectra ====
%
% Presented below are the final spectra that are used in the DM analysis. Bluer spectra are for lower DM mass models. The redder, the higher the DM mass.
% Energy (x-axis) was chosen to roughly represent the energy sensitivity of NST.
% These spectra are the composite (nu and tau flavors) versions. How these are combined is mentioned earlier.
%
% <center>
% {|class="wikitable" style="border-spacing: 2px; border: 1px solid darkgray;"
% |+ Output composite spectra after spline fit and line smoothing
% |-
% | <center> $ \chi\chi \rightarrow b\bar{b} $ </center>
% | <center> $ \chi\chi \rightarrow t\bar{t} $ </center>
% | <center> $ \chi\chi \rightarrow \nu_{\mu}\overline{\nu_{\mu}} $ </center>
% |-
% | [[Image:bb_rainbow.png |300px]]
% | [[Image:tt_rainbow.png |300px]]
% | [[Image:numu_rainbow.png |300px]]
% |-
% | <center> $ \chi\chi \rightarrow W^+ W^- $ </center>
% | <center> $ \chi\chi \rightarrow Z^0Z^0 $ </center>
% | <center> $ \chi\chi \rightarrow \nu_{\tau}\overline{\nu_{\tau}} $ </center>
% |-
% | [[Image:ww_rainbow.png |300px]]
% | [[Image:zz_rainbow.png |300px]]
% | [[Image:nutau_rainbow.png |300px]]
% |-
% | <center> $ \chi\chi \rightarrow \nu_{e}\overline{\nu_{e}} $ </center>
% |-
% | [[Image:nue_rainbow.png |300px]]
% |}
% </center>
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{\J - Astrophysical Component}\label{sec:icDM_spatialmodel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% === DM distribution profile (J-factor): $ J $ ===
%
% The expected neutrino counts from a dwarf spheroidal galaxy depends also on the the 'astrophysical factor'. The value for this (in our specific case) J-factor for a target depends on its dark matter density distribution, $\rho_{\chi}$ and how far it is $ l $. For this analysis, we adopt dark matter halo models for galaxy clusters exclusively from [https://iopscience.iop.org/article/10.1088/0004-637X/801/2/74 A. Geringer-Sameth et al, Astrophys. J. 801 no. 2, (2015) 74.]. These models are based on a modified Navarro-Frenk-While (NFW) profile where the indices of the NFW (traditionally 1,3,1) are allowed to float. More specifically, these DM distributions are described using the Zhao profile.  The Zhao profile is written as:
%
% <center>
% $
% \rho_{Zhao}(r; r_{s}, \rho_{s}, \alpha, \beta, \gamma) = \frac{\rho_{s}}{\left(\frac{r}{r_{s}}\right)^{\gamma} \left[1 + \left(\frac{r}{r_{s}}\right)^{\alpha} \right]^{(\beta-\gamma)/\alpha}  }
% $,
% </center>
% where $ r_{s} $, $\rho_{s}$ denote scale radius, scale density respectively. According to these models, the dark matter distributions of the sources are spherically symmetric. Hence, the J-factor is calculated as the following :
% <center>
% $
%
% J(\theta) = 2 \pi \int_{0}^{\theta} \sin \theta^{'} d\theta^{'} \int^{\infty}_{0}dl^{'} \rho^2_{\chi}(r[l^{'}, \theta^{'}])
%
% $,
% </center>
% where $ \theta $ is the angular distance from the center of the source. For the case annihilation, the source diameter, [https://iopscience.iop.org/article/10.1088/0004-637X/801/2/74 here] defined as the $ 2 \theta_{\mathrm{max}} $ , of these dwarves is typically under $ 1^{\circ} $ with the largest in the catalog, Fornax, extending to $ 2.61^{\circ} $. Fornax is not in the northern sky and the remaining sources are notably below this angular size. Therefore, the sources are treated as point sources because the typical source diameter is under 1 degree. The J-factor used for the point source assumption is the total J emitted from $ \theta_{\mathrm{max}} $. These values are enumerated in Geringer-Sameth 2015 and again in the table below with their coordinates. Coordinates are given in J200.0 equatorial coordinates.
%
% <center>
% {| class="wikitable"
% |-
% ! Source
% ! J-factor $ (\mathrm{GeV}^2 \mathrm{cm}^{-5}) $
% ! RA $ (^\circ) $
% ! Dec $ (^\circ) $
% |-
% | BootesI
% | 1.720e+18
% | 210.03
% | 14.49
% |-
% | Canes Venatici I
% | 2.730e+17
% | 202.01
% | 33.56
% |-
% | Canes Venatici II
% | 4.473e+17
% | 194.29
% | 34.32
% |-
% | Coma Berenices
% | 1.056e+19
% | 186.74
% | 23.90
% |-
% | Draco
% | 1.127e+19
% | 260.06
% | 57.07
% |-
% | Hercules
% | 7.326e+16
% | 247.72
% | 12.75
% |-
% | Leo I
% | 6.942e+17
% | 152.11
% | 12.29
% |-
% | Leo II
% | 9.311e+17
% | 168.34
% | 22.13
% |-
% | Leo V
% | 2.343e+16
% | 172.79
% | 2.22
% |-
% | Leo T
% | 1.288e+17
% | 143.72
% | 17.05
% |-
% | Segue 1
% | 2.267e+19
% | 151.75
% | 16.06
% |-
% | Segue 2
% | 1.6154e+16
% | 34.81
% | 20.17
% |-
% | Ursa Major I
% | 7.459e+17
% | 158.72
% | 51.94
% |-
% | Ursa Major II
% | 2.655e+19
% | 132.77
% | 63.11
% |-
% | Ursa Minor
% | 8.834e+18
% | 227.24
% | 67.24
% |}
% </center>
%

%
% === Statistical Methods ===
%
% I use the Point-Source search likelihood which is widely used in IceCube analyses.
% The likelihood function is defined as the following:
%
% <center>
% $
% L(n_{s}) = \prod^{N}_{i=1}\left[\frac{n_{s}}{N}S_{i} + \left( 1 - \frac{n_{s}}{N} \right) B_{i} \right],
% $
% </center>
%
% where  $ i $ is an event index, $S$ and $B$ are the signal PDF and background PDF respectively. For a joint analysis where the sources are stacked the likelihood is expanded in the simplified way:
%
% <center>
% $
% L(n_{s}) = \prod^{N_\mathrm{sources}}_{i=1} L_i(n_{s}) ,
% $
% </center>
%
% Where $ L_i $ is the likelihood from the i-th source in the stacked analysis.
%
% === Test Statistic (TS) Distributions ===
%
% The test statistic is defined as a likelihood ratio,
% <center>
% $
% \mathrm{TS} = -2\ln\frac{L(0)}{L(\hat{n}_{s})}
% $,
% </center>
% where $\hat{n}_{s}$ is the best-fit value for $n_{s}$.
%
% The TS distributions are not expected to behalf according to a chi-squared distribution with 1 degree of freedom.
% This is in large part due to the distinct spectral shapes demonstrated earlier.
% These can vary significantly between DM mass and annihilation models.
% Therefor, Wilks' theorem may not be applicable to the analysis.
% Instead, a critical value is defined from a large number of background trials.
%
% I assume that TS values are physical: $ \mathrm{TS} \ge 0 $.
% $\eta$ denotes the fraction of positive TS values above the threshold and written in the legend.
% $ \epsilon[x] $ indicate the fraction of events where $ \mathrm{TS} < x $. For TS plots shown here, the decimal values of x are 1.0e-2 and 1.0e-3.
% The following plots show the background TS distributions obtained from Segue1, a source with little Earth attenuation and large J-factor, assuming that dark matter annihilates into $b\overline{b}$.
% I also show the 15 source stack TS distributions with identical DM models.
%
% ==== TS per Source ====
%
% Below I present the TS distributions for Segue1 and $ \chi\chi \rightarrow b\overline{b}$. '''All remaining channels and source TS panels are hosted on [[DM_ann_from_dSph_TSperSource | this page]]'''
%
% Although it was not expected, almost every distribution produced follows a chi2 distribution with 1 degree of freedom.
% This is important for future assumptions made (in multi-messenger) and may justify statistical calculations assuming Wilk's theorem is valid.
%
% <center>
% [[Image: Segue1_bb_chi2_Masspanel.jpeg| 800px ]]
%
% Each subplot, except the final, is the TS distribution for a specific DM mass listed in the subplot. The final subplot plots the all DM spectral models used as input for the TS distribution calculations with bluer lines indicating lower DM mass and redder indicating higher DM mass.
% </center>
%
% ==== stacked TS ====
%
% The presentation of these plots are identical to the previous 'per Source' section. I use csky source software to calculate the TS distributions.
% Bugs were found when implementing, however were rectified. Warning to future users performing a stacked analysis with custom spectra.
% In using the above, I am making the implicit assumption that the primary/only cause to a difference in neutrino counts from the sources is accounted for through the J-factors.
% The J-factors are therefor used as weights for the stacking where an individual source's weight is defined as:
%
% <center>
% <big>$ w_i = \frac{J_i}{\sum^{N_\mathrm{sources}}_{k=1}J_k} $</big>
% </center>
%
% where $ w_i $ is the ith source, $ J_{i/k} $ is the i/k-th source's J-factor.
%
% Below is the TS distribution for each SM annihilation channel with stacking of 15 sources.
%
% [[Media: stacked_bb_chi2_Masspanel.pdf | $ b\overline{b} $ 1/7/2024]]
%
% [[Media: stacked_ww_chi2_Masspanel.pdf | $W^+W^- $ 1/7/2024]]
%
% [[Media: stacked_zz_chi2_Masspanel.pdf | $Z^0 Z^0 $ 1/7/2024]]
%
% [[Media: stacked_tt_chi2_Masspanel.pdf | $ t\overline{t} $ 1/7/2024]]
%
% [[Media: stacked_nuenue_chi2_Masspanel.pdf | $ \nu_e\overline{\nu_e} $ 1/7/2024]]
%
% [[Media: stacked_numunumu_chi2_Masspanel.pdf | $ \nu_\mu\overline{\nu_\mu} $ 1/7/2024]]
%
% [[Media: stacked_nutaunutau_chi2_Masspanel.pdf | $ \nu_\tau\overline{\nu_\tau} $ 1/7/2024]]
%
% Each subplot, except the final, is the TS distribution for a specific DM mass listed in the subplot. The final subplot plots the all DM spectral models used as input for the TS distribution calculations with bluer lines indicating lower DM mass and redder indicating higher DM mass.
% Below is an image of bb. The full resolution pdfs were provided in links above.
%
% <center>
% [[Image: stacked_bb_chi2_Masspanel.jpeg| 800px ]]
%
% </center>
%
% === Signal Recovery ===
%
% This did have a lot of plots before but the spectra have change substantially since then. They converge well but since the computational is expensive here, I'm holding off on rerunning until the spectra are finalized.
%
% === Sensitivities ===
%
% In IceCube, we usually define the 90% confidence level (CL), as the minimum number of signal events (n_s) required to have a Type I error rate smaller than 0.5 and Type II error rate of 0.1.
% Csky performs the sweep to find n_s that satisfies the previous condition, and from n_s I use the following equation
%
% <center>
% $
% n_{s} = T_{live} \int_{0}^{\Delta\Omega}d\Omega \int_{E_{min}}^{E_{max}} dE_{\nu} A_{eff}(\hat{n}, E_{\nu}) \frac{d \Phi_{\nu}}{d\Omega d E_{\nu}} (\hat{n}, E_{\nu}),
% $
% </center>
%
% to extract the sensitivity on the dark matter annihilation cross-section.
% $ T_{live} $ is the detector livetime, $ A_{eff} $ is the effective area of the detector, and $ E_{min} $, $ E_{max} $ are the minimum, maximum energies of the expected neutrinos, respectively.
%
% Sensitivities are calculated for each source individually as if they were the only source and as a stack.
% Example plots of these plots are shown below and organized by the single source/stacked studies.
% Finally, I generated a plot with all hypotheses which is presented at the very end.
%
% ==== per Source ====
%
% ==== Stacked ====
%